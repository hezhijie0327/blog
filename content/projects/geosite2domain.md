---
title: "Geosite2Domain - åœ°ç†ç«™ç‚¹åŸŸåè½¬æ¢å·¥å…·"
description: "å°†Geositeæ ¼å¼è½¬æ¢ä¸ºæ˜Žæ–‡åŸŸåè§„åˆ™çš„å®žç”¨å·¥å…·ï¼Œæ”¯æŒå¤šç§ç½‘ç»œè®¾å¤‡"
date: "2025-07-29"
type: "personal"
tags: ["Shell", "Geosite", "Domain Rules", "Network", "Proxy", "Router"]
image: "https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=800&h=400&fit=crop"
link: "https://github.com/hezhijie0327/Geosite2Domain"
---

# Geosite2Domain - åœ°ç†ç«™ç‚¹åŸŸåè½¬æ¢å·¥å…·

ä¸€ä¸ªä¸“é—¨ç”¨äºŽå°†Geositeæ ¼å¼è½¬æ¢ä¸ºæ˜Žæ–‡åŸŸåè§„åˆ™çš„å·¥å…·ï¼Œä¸ºç½‘ç»œä»£ç†å’Œè·¯ç”±ä¼˜åŒ–æä¾›æ ‡å‡†åŒ–çš„åŸŸååˆ—è¡¨ã€‚

## ðŸŽ¯ é¡¹ç›®æ¦‚è¿°

Geosite2Domainè‡´åŠ›äºŽè§£å†³ä¸åŒç½‘ç»œè®¾å¤‡å¯¹åŸŸåè§„åˆ™æ ¼å¼çš„å…¼å®¹æ€§é—®é¢˜ï¼Œæä¾›æ ‡å‡†åŒ–çš„åŸŸåè§„åˆ™è½¬æ¢æœåŠ¡ã€‚

### æ ¸å¿ƒåŠŸèƒ½
- ðŸ”„ **æ ¼å¼è½¬æ¢** - Geositeåˆ°æ˜Žæ–‡åŸŸåçš„é«˜æ•ˆè½¬æ¢
- ðŸ“Š **åˆ†ç±»ç®¡ç†** - æŒ‰ç±»åˆ«å’Œç”¨é€”å¯¹åŸŸåè¿›è¡Œåˆ†ç±»
- âš¡ **é«˜æ€§èƒ½å¤„ç†** - ä¼˜åŒ–çš„ç®—æ³•å¤„ç†å¤§è§„æ¨¡åŸŸåæ•°æ®
- ðŸ”§ **å¤šè®¾å¤‡å…¼å®¹** - æ”¯æŒè·¯ç”±å™¨ã€ä»£ç†è½¯ä»¶ç­‰å¤šç§è®¾å¤‡
- ðŸ“ˆ **è‡ªåŠ¨æ›´æ–°** - å®šæœŸæ›´æ–°åŸŸåæ•°æ®åº“

## ðŸ—ï¸ æŠ€æœ¯æž¶æž„

### æ•°æ®å¤„ç†æµç¨‹
```mermaid
graph TD
    A[Geositeæ•°æ®æº] --> B[æ•°æ®è§£æž]
    B --> C[åŸŸåæå–]
    C --> D[é‡å¤åŽ»é™¤]
    D --> E[æ ¼å¼éªŒè¯]
    E --> F[åˆ†ç±»æ•´ç†]
    F --> G[å¤šæ ¼å¼è¾“å‡º]

    H[V2Rayé¡¹ç›®] --> A
    I[å…¶ä»–geositeæº] --> A

    G --> J[æ˜Žæ–‡æ ¼å¼]
    G --> K[è·¯ç”±å™¨æ ¼å¼]
    G --> L[ä»£ç†è½¯ä»¶æ ¼å¼]
    G --> M[é˜²ç«å¢™æ ¼å¼]
```

## ðŸŽ¨ æ ¸å¿ƒåŠŸèƒ½

### 1. Geositeè§£æžå™¨
```bash
#!/bin/bash
# geosite_parser.sh - Geositeæ–‡ä»¶è§£æžå™¨

#!/bin/bash

# Geositeè§£æžå‡½æ•°
parse_geosite() {
    local geosite_file=$1
    local output_dir=$2

    echo "Parsing geosite file: $geosite_file"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p "$output_dir"

    # ä½¿ç”¨v2ctlè§£æžgeositeæ–‡ä»¶
    if command -v v2ctl >/dev/null 2>&1; then
        v2ctl geosite "$geosite_file" -o "$output_dir/parsed.json"
    else
        # å¤‡ç”¨è§£æžæ–¹æ³•
        python3 parse_geosite.py "$geosite_file" "$output_dir"
    fi

    # æå–åŸŸååˆ—è¡¨
    extract_domains "$output_dir/parsed.json" "$output_dir"
}

# åŸŸåæå–å‡½æ•°
extract_domains() {
    local json_file=$1
    local output_dir=$2

    echo "Extracting domains from JSON..."

    # ä½¿ç”¨jqæå–åŸŸå
    if command -v jq >/dev/null 2>&1; then
        jq -r '.[].list[]' "$json_file" | sort -u > "$output_dir/domains_raw.txt"
    else
        # å¤‡ç”¨æå–æ–¹æ³•
        python3 extract_domains.py "$json_file" "$output_dir"
    fi

    # åŽ»é™¤é‡å¤å’Œæ— æ•ˆåŸŸå
    clean_domains "$output_dir/domains_raw.txt" "$output_dir/domains_clean.txt"
}

# åŸŸåæ¸…ç†å‡½æ•°
clean_domains() {
    local input_file=$1
    local output_file=$2

    echo "Cleaning domains..."

    # åŽ»é™¤ç©ºè¡Œå’Œæ³¨é‡Š
    grep -v '^#' "$input_file" | grep -v '^$' | \
    # åŽ»é™¤é‡å¤
    sort -u | \
    # éªŒè¯åŸŸåæ ¼å¼
    grep -E '^[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$' | \
    # è½¬æ¢ä¸ºå°å†™
    tr '[:upper:]' '[:lower:]' > "$output_file"

    local cleaned_count=$(wc -l < "$output_file")
    echo "Cleaned $cleaned_count unique domains"
}
```

### 2. åˆ†ç±»ç®¡ç†å™¨
```bash
#!/bin/bash
# classifier.sh - åŸŸååˆ†ç±»ç®¡ç†

# é¢„å®šä¹‰åˆ†ç±»
declare -A CATEGORIES=(
    ["social"]="facebook.com,twitter.com,instagram.com,linkedin.com"
    ["streaming"]="youtube.com,netflix.com,twitch.tv,spotify.com"
    ["search"]="google.com,bing.com,duckduckgo.com"
    ["shopping"]="amazon.com,ebay.com,taobao.com,tmall.com"
    ["tech"]="github.com,stackoverflow.com,microsoft.com,apple.com"
    ["news"]="cnn.com,bbc.com,nytimes.com,washingtonpost.com"
    ["gaming"]="steam.com,epicgames.com,roblox.com,minecraft.net"
)

# è‡ªåŠ¨åˆ†ç±»å‡½æ•°
auto_classify() {
    local domains_file=$1
    local output_dir=$2

    echo "Auto-classifying domains..."

    # åˆ›å»ºåˆ†ç±»ç›®å½•
    mkdir -p "$output_dir/categories"

    # ä¸ºæ¯ä¸ªåˆ†ç±»åˆ›å»ºæ–‡ä»¶
    for category in "${!CATEGORIES[@]}"; do
        local domains=${CATEGORIES[$category]}
        local output_file="$output_dir/categories/${category}.txt"

        echo "Processing category: $category"

        # æå–ç›¸å…³åŸŸå
        while IFS=',' read -ra DOMAIN_LIST; do
            for domain in "${DOMAIN_LIST[@]}"; do
                grep "^$domain$" "$domains_file" >> "$output_file" 2>/dev/null || true

                # æŸ¥æ‰¾å­åŸŸå
                grep "\.$domain$" "$domains_file" >> "$output_file" 2>/dev/null || true
            done
        done <<< "$domains"

        # åŽ»é‡å¹¶æŽ’åº
        sort -u "$output_file" > "$output_file.tmp"
        mv "$output_file.tmp" "$output_file"

        local count=$(wc -l < "$output_file")
        echo "  $category: $count domains"
    done

    # åˆ›å»ºæœªåˆ†ç±»åŸŸååˆ—è¡¨
    comm -23 <(sort "$domains_file") <(cat "$output_dir/categories"/*.txt | sort -u) > "$output_dir/categories/uncategorized.txt"
}

# æ‰‹åŠ¨åˆ†ç±»é…ç½®
manual_classify() {
    local domains_file=$1
    local config_file=$2

    echo "Applying manual classification rules..."

    while IFS= read -r line; do
        # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
        [[ $line =~ ^#.*$ ]] && continue
        [[ -z $line ]] && continue

        # è§£æžè§„åˆ™: category:domain_pattern
        if [[ $line =~ ^([^:]+):(.+)$ ]]; then
            local category="${BASH_REMATCH[1]}"
            local pattern="${BASH_REMATCH[2]}"

            echo "Rule: $category -> $pattern"

            # åº”ç”¨è§„åˆ™
            grep -E "$pattern" "$domains_file" >> "categories/${category}.txt"
        fi
    done < "$config_file"
}
```

### 3. å¤šæ ¼å¼è¾“å‡ºå™¨
```bash
#!/bin/bash
# formatter.sh - å¤šæ ¼å¼è¾“å‡º

# ç”Ÿæˆæ˜Žæ–‡æ ¼å¼
generate_plain_text() {
    local input_file=$1
    local output_file=$2

    echo "# Domain list generated on $(date)" > "$output_file"
    echo "# Total domains: $(wc -l < "$input_file")" >> "$output_file"
    echo "" >> "$output_file"
    cat "$input_file" >> "$output_file"
}

# ç”Ÿæˆè·¯ç”±å™¨æ ¼å¼ (dnsmasq)
generate_dnsmasq() {
    local input_file=$1
    local output_file=$2

    echo "# dnsmasq configuration" > "$output_file"
    echo "# Generated on $(date)" >> "$output_file"
    echo "" >> "$output_file"

    while IFS= read -r domain; do
        echo "server=/$domain/8.8.8.8" >> "$output_file"
        echo "server=/$domain/1.1.1.1" >> "$output_file"
    done < "$input_file"
}

# ç”Ÿæˆä»£ç†è½¯ä»¶æ ¼å¼ (Clash)
generate_clash() {
    local input_file=$1
    local output_file=$2

    cat > "$output_file" << EOF
# Clash domain rules
# Generated on $(date)

payload:
EOF

    while IFS= read -r domain; do
        echo "  - '$domain'" >> "$output_file"
    done < "$input_file"
}

# ç”Ÿæˆé˜²ç«å¢™æ ¼å¼ (iptables)
generate_iptables() {
    local input_file=$1
    local output_file=$2

    echo "# iptables rules" > "$output_file"
    echo "# Generated on $(date)" >> "$output_file"
    echo "" >> "$output_file"

    while IFS= read -r domain; do
        echo "# Allow traffic to $domain" >> "$output_file"
        echo "iptables -A OUTPUT -d $domain -j ACCEPT" >> "$output_file"
    done < "$input_file"
}

# æ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ ¼å¼
generate_all_formats() {
    local input_file=$1
    local output_dir=$2
    local base_name=$3

    mkdir -p "$output_dir"

    echo "Generating all format files..."

    generate_plain_text "$input_file" "$output_dir/${base_name}.txt"
    generate_dnsmasq "$input_file" "$output_dir/${base_name}_dnsmasq.conf"
    generate_clash "$input_file" "$output_dir/${base_name}_clash.yaml"
    generate_iptables "$input_file" "$output_dir/${base_name}_iptables.sh"

    echo "Format generation completed!"
}
```

### 4. Pythonè¾…åŠ©è„šæœ¬
```python
# geosite_parser.py - Pythonç‰ˆGeositeè§£æžå™¨
import json
import re
from typing import List, Dict, Set
from pathlib import Path

class GeositeParser:
    def __init__(self):
        self.domains: Set[str] = set()
        self.categories: Dict[str, List[str]] = {}

    def parse_geosite_file(self, file_path: str) -> Dict:
        """è§£æžgeositeæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            for site in data:
                site_name = site.get('siteCode', '')
                domain_list = site.get('list', [])

                self.categories[site_name] = domain_list
                self.domains.update(domain_list)

            return {
                'total_domains': len(self.domains),
                'categories': len(self.categories),
                'categories_detail': {
                    name: len(domains) for name, domains in self.categories.items()
                }
            }

        except Exception as e:
            print(f"Error parsing geosite file: {e}")
            return {}

    def extract_domains(self, output_file: str):
        """æå–æ‰€æœ‰åŸŸååˆ°æ–‡ä»¶"""
        sorted_domains = sorted(self.domains)

        with open(output_file, 'w', encoding='utf-8') as f:
            for domain in sorted_domains:
                f.write(f"{domain}\n")

    def validate_domain(self, domain: str) -> bool:
        """éªŒè¯åŸŸåæ ¼å¼"""
        if not domain or len(domain) > 253:
            return False

        # åŸŸåæ­£åˆ™è¡¨è¾¾å¼
        pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$'
        return bool(re.match(pattern, domain))

    def filter_domains(self, pattern: str) -> List[str]:
        """æ ¹æ®æ¨¡å¼è¿‡æ»¤åŸŸå"""
        return [domain for domain in self.domains if re.search(pattern, domain, re.IGNORECASE)]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    parser = GeositeParser()

    # è§£æžgeositeæ–‡ä»¶
    stats = parser.parse_geosite_file("geosite.dat")
    print(f"è§£æžå®Œæˆ: {stats['total_domains']} ä¸ªåŸŸå, {stats['categories']} ä¸ªåˆ†ç±»")

    # æå–åŸŸå
    parser.extract_domains("domains_all.txt")

    # è¿‡æ»¤ç¤ºä¾‹
    google_domains = parser.filter_domains(r'\.google\.com$')
    print(f"Googleç›¸å…³åŸŸå: {len(google_domains)} ä¸ª")
```

## ðŸ”§ è‡ªåŠ¨åŒ–å·¥ä½œæµ

### GitHub Actionsè‡ªåŠ¨åŒ–
```yaml
# .github/workflows/update.yml
name: Update Geosite Rules

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©å‡Œæ™¨2ç‚¹æ›´æ–°
  workflow_dispatch:
  push:
    branches: [main]

jobs:
  update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq curl wget

      - name: Download latest geosite
        run: |
          # ä¸‹è½½æœ€æ–°çš„geositeæ–‡ä»¶
          wget -O geosite.dat https://github.com/v2fly/domain-list-community/releases/latest/download/dlc.dat

      - name: Parse and convert
        run: |
          chmod +x scripts/*.sh
          ./scripts/geosite_parser.sh geosite.dat output/
          ./scripts/classifier.sh output/domains_clean.txt output/
          ./scripts/formatter.sh output/domains_clean.txt output/ geosite

      - name: Generate statistics
        run: |
          python3 scripts/generate_stats.py output/ > stats.json

      - name: Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add output/
          git add stats.json
          git diff --staged --quiet || git commit -m "Auto update geosite rules - $(date +'%Y-%m-%d')"
          git push

      - name: Create release
        if: contains(github.event.head_commit.message, '[release]')
        run: |
          tag_name="v$(date +'%Y%m%d')"
          gh release create "$tag_name" \
            --title "Geosite Rules $tag_name" \
            --generate-notes \
            output/
```

## ðŸ“Š ç»Ÿè®¡å’Œåˆ†æž

### åŸŸåç»Ÿè®¡
```python
# stats.py - åŸŸåç»Ÿè®¡åˆ†æž
import json
from collections import Counter
import matplotlib.pyplot as plt

class DomainStats:
    def __init__(self, domains_file: str):
        self.domains_file = domains_file
        self.domains = self.load_domains()

    def load_domains(self) -> list:
        """åŠ è½½åŸŸååˆ—è¡¨"""
        with open(self.domains_file, 'r') as f:
            return [line.strip() for line in f if line.strip()]

    def get_tld_distribution(self) -> Counter:
        """èŽ·å–é¡¶çº§åŸŸååˆ†å¸ƒ"""
        tlds = []
        for domain in self.domains:
            parts = domain.split('.')
            if len(parts) > 1:
                tlds.append(parts[-1])

        return Counter(tlds)

    def get_domain_length_stats(self) -> dict:
        """èŽ·å–åŸŸåé•¿åº¦ç»Ÿè®¡"""
        lengths = [len(domain) for domain in self.domains]
        return {
            'min': min(lengths),
            'max': max(lengths),
            'avg': sum(lengths) / len(lengths),
            'median': sorted(lengths)[len(lengths) // 2]
        }

    def generate_report(self) -> dict:
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        tld_dist = self.get_tld_distribution()
        length_stats = self.get_domain_length_stats()

        return {
            'total_domains': len(self.domains),
            'tld_distribution': dict(tld_dist.most_common(20)),
            'domain_length_stats': length_stats,
            'unique_tlds': len(tld_dist)
        }

    def plot_tld_distribution(self, output_file: str):
        """ç»˜åˆ¶TLDåˆ†å¸ƒå›¾"""
        tld_dist = self.get_tld_distribution()
        top_20 = dict(tld_dist.most_common(20))

        plt.figure(figsize=(12, 6))
        plt.bar(top_20.keys(), top_20.values())
        plt.title('Top 20 TLD Distribution')
        plt.xlabel('TLD')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(output_file)
        plt.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    stats = DomainStats("output/domains_clean.txt")
    report = stats.generate_report()

    print("Domain Statistics Report:")
    print(f"Total domains: {report['total_domains']}")
    print(f"Unique TLDs: {report['unique_tlds']}")
    print(f"Average domain length: {report['domain_length_stats']['avg']:.2f}")

    # ç”Ÿæˆå›¾è¡¨
    stats.plot_tld_distribution("tld_distribution.png")

    # ä¿å­˜æŠ¥å‘Š
    with open("domain_stats.json", "w") as f:
        json.dump(report, f, indent=2)
```

## ðŸš€ ä½¿ç”¨ç¤ºä¾‹

### å¿«é€Ÿå¼€å§‹
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/hezhijie0327/Geosite2Domain.git
cd Geosite2Domain

# ä¸‹è½½æœ€æ–°geositeæ–‡ä»¶
wget https://github.com/v2fly/domain-list-community/releases/latest/download/dlc.dat -O geosite.dat

# è§£æžå’Œè½¬æ¢
chmod +x scripts/*.sh
./scripts/geosite_parser.sh geosite.dat output/

# åˆ†ç±»æ•´ç†
./scripts/classifier.sh output/domains_clean.txt output/

# ç”Ÿæˆå¤šç§æ ¼å¼
./scripts/formatter.sh output/domains_clean.txt output/ geosite

# æŸ¥çœ‹ç»“æžœ
ls -la output/
```

### è‡ªå®šä¹‰åˆ†ç±»
```bash
# åˆ›å»ºè‡ªå®šä¹‰åˆ†ç±»é…ç½®
cat > custom_rules.txt << EOF
# ç¤¾äº¤åª’ä½“
social:.*facebook\.com$
social:.*twitter\.com$
social:.*instagram\.com$

# è§†é¢‘æµåª’ä½“
streaming:.*youtube\.com$
streaming:.*netflix\.com$
streaming:.*twitch\.tv$

# å¼€å‘å·¥å…·
dev:.*github\.com$
dev:.*stackoverflow\.com$
dev:.*docker\.com$
EOF

# åº”ç”¨è‡ªå®šä¹‰åˆ†ç±»
./scripts/manual_classify.sh output/domains_clean.txt custom_rules.txt
```

## ðŸ”® é¡¹ç›®ä»·å€¼

### æŠ€æœ¯ä»·å€¼
- **æ ‡å‡†åŒ–è½¬æ¢** - æä¾›geositeåˆ°åŸŸåçš„æ ‡å‡†è½¬æ¢æ–¹æ¡ˆ
- **å¤šæ ¼å¼æ”¯æŒ** - æ”¯æŒå„ç§ç½‘ç»œè®¾å¤‡å’Œè½¯ä»¶çš„æ ¼å¼éœ€æ±‚
- **è‡ªåŠ¨åŒ–å¤„ç†** - å®Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®æ›´æ–°å’Œå¤„ç†æµç¨‹
- **è´¨é‡ä¿éšœ** - å®Œå–„çš„æ•°æ®éªŒè¯å’Œæ¸…ç†æœºåˆ¶

### å®žç”¨ä»·å€¼
- **ç½‘ç»œä¼˜åŒ–** - ä¸ºè·¯ç”±ä¼˜åŒ–å’Œä»£ç†é…ç½®æä¾›åŸºç¡€æ•°æ®
- **è®¾å¤‡å…¼å®¹** - è§£å†³ä¸åŒè®¾å¤‡é—´çš„åŸŸåè§„åˆ™å…¼å®¹æ€§é—®é¢˜
- **ç»´æŠ¤ä¾¿åˆ©** - ç®€åŒ–ç½‘ç»œè®¾å¤‡ç»´æŠ¤å’Œé…ç½®æ›´æ–°
- **æ€§èƒ½æå‡** - é€šè¿‡ç²¾å‡†çš„åŸŸååˆ†ç±»æå‡ç½‘ç»œæ€§èƒ½

### ç¤¾åŒºå½±å“
- ðŸ“š **å¼€æºè´¡çŒ®** - ä¸ºç½‘ç»œæŠ€æœ¯ç¤¾åŒºæä¾›å®žç”¨å·¥å…·
- ðŸ”„ **æŒç»­æ›´æ–°** - è·Ÿéšç½‘ç»œçŽ¯å¢ƒå˜åŒ–åŠæ—¶æ›´æ–°
- ðŸ’¡ **æŠ€æœ¯åˆ›æ–°** - æŽ¨åŠ¨ç½‘ç»œé…ç½®è‡ªåŠ¨åŒ–æŠ€æœ¯å‘å±•
- ðŸŒ **å…¨çƒé€‚ç”¨** - æ”¯æŒå…¨çƒèŒƒå›´å†…çš„ç½‘ç»œä¼˜åŒ–éœ€æ±‚

---

**é¡¹ç›®é“¾æŽ¥**: [GitHub Repository](https://github.com/hezhijie0327/Geosite2Domain)

**æŠ€æœ¯æ ˆ**: Shell Script | Python | Geosite | Domain Rules | Network Automation