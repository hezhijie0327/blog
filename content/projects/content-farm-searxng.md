---
title: "ContentFarm4SearXNG - å†…å®¹å†œåœºè¿‡æ»¤å™¨"
description: "ä¸ºSearXNGæœç´¢å¼•æ“æä¾›å†…å®¹å†œåœºhostnameè§„åˆ™ï¼Œæå‡æœç´¢ç»“æœè´¨é‡"
date: "2025-07-20"
type: "personal"
tags: ["Python", "SearXNG", "Content Farm", "Search Engine", "Filter", "uBlock Origin"]
image: "https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=800&h=400&fit=crop"
link: "https://github.com/hezhijie0327/ContentFarm4SearXNG"
---

# ContentFarm4SearXNG - å†…å®¹å†œåœºè¿‡æ»¤å™¨

ä¸€ä¸ªä¸“ä¸ºSearXNGå…ƒæœç´¢å¼•æ“è®¾è®¡çš„å†…å®¹å†œåœºè¿‡æ»¤å·¥å…·ï¼Œé€šè¿‡è¯†åˆ«å’Œè¿‡æ»¤ä½è´¨é‡å†…å®¹å†œåœºç½‘ç«™æ¥æå‡æœç´¢ç»“æœè´¨é‡ã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

ContentFarm4SearXNGè‡´åŠ›äºè§£å†³æœç´¢å¼•æ“ä¸­çš„å†…å®¹å†œåœºé—®é¢˜ï¼Œä¸ºç”¨æˆ·æä¾›æ›´é«˜è´¨é‡ã€æ›´å¯é çš„æœç´¢ç»“æœã€‚

### æ ¸å¿ƒä»·å€¼
- ğŸ” **æå‡æœç´¢è´¨é‡** - è¿‡æ»¤ä½è´¨é‡å†…å®¹å†œåœº
- ğŸ›¡ï¸ **ä¿æŠ¤ç”¨æˆ·ä½“éªŒ** - å‡å°‘è¯¯å¯¼æ€§å’Œä½ä»·å€¼ä¿¡æ¯
- ğŸ“Š **æ•°æ®é©±åŠ¨** - åŸºäºå¤šæºæ•°æ®åˆ†æè¯†åˆ«å†…å®¹å†œåœº
- ğŸ”„ **æŒç»­ç»´æŠ¤** - å®šæœŸæ›´æ–°è¿‡æ»¤è§„åˆ™
- ğŸŒ **å¤šå¹³å°æ”¯æŒ** - å…¼å®¹SearXNGå’ŒuBlock Origin

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### è¯†åˆ«ç®—æ³•
```python
import re
import requests
from typing import List, Dict, Set

class ContentFarmDetector:
    def __init__(self):
        self.content_farm_patterns = [
            # åŸºäºURLæ¨¡å¼çš„è¯†åˆ«
            r'.*\.info$',
            r'.*\.top$',
            r'.*-blog\..*',
            r'.*guide.*',

            # åŸºäºå†…å®¹ç‰¹å¾çš„è¯†åˆ«
            r'.*(how|guide|tutorial).*step.*',
            r'.*(best|top).*(\d+).*',
            r'.*(review|rating).*',
        ]

        self.low_quality_indicators = [
            'clickbait',
            'sponsored content',
            'advertisement',
            'affiliate links',
            'pop-up ads',
        ]

    def analyze_domain(self, domain: str) -> Dict:
        """åˆ†æåŸŸåæ˜¯å¦ä¸ºå†…å®¹å†œåœº"""
        result = {
            'domain': domain,
            'is_content_farm': False,
            'confidence': 0.0,
            'reasons': []
        }

        # æ¨¡å¼åŒ¹é…æ£€æµ‹
        for pattern in self.content_farm_patterns:
            if re.match(pattern, domain, re.IGNORECASE):
                result['is_content_farm'] = True
                result['confidence'] += 0.3
                result['reasons'].append(f'Match pattern: {pattern}')

        # å†…å®¹è´¨é‡åˆ†æ
        try:
            response = requests.get(f'https://{domain}', timeout=10)
            content = response.text.lower()

            for indicator in self.low_quality_indicators:
                if indicator in content:
                    result['confidence'] += 0.2
                    result['reasons'].append(f'Content indicator: {indicator}')

        except Exception as e:
            result['reasons'].append(f'Analysis failed: {str(e)}')

        result['is_content_farm'] = result['confidence'] > 0.5
        return result

    def generate_filter_rules(self, domains: List[str]) -> str:
        """ç”Ÿæˆè¿‡æ»¤è§„åˆ™æ–‡ä»¶"""
        rules = []
        rules.append("# Content Farm Filter Rules for SearXNG")
        rules.append("# Generated on: " + str(datetime.now()))
        rules.append("")

        for domain in domains:
            analysis = self.analyze_domain(domain)
            if analysis['is_content_farm']:
                rules.append(f"# {domain} - {', '.join(analysis['reasons'])}")
                rules.append(f"! {domain}")
                rules.append("")

        return '\n'.join(rules)

# ä½¿ç”¨ç¤ºä¾‹
detector = ContentFarmDetector()
test_domains = ['example.com', 'blog.example.top', 'guide-site.info']

for domain in test_domains:
    result = detector.analyze_domain(domain)
    print(f"Domain: {result['domain']}")
    print(f"Content Farm: {result['is_content_farm']}")
    print(f"Confidence: {result['confidence']:.2f}")
    print(f"Reasons: {', '.join(result['reasons'])}")
    print("-" * 50)
```

### å¤šæ ¼å¼è¾“å‡º
```python
class FilterGenerator:
    def __init__(self, detector: ContentFarmDetector):
        self.detector = detector

    def generate_searxng_rules(self, domains: List[str]) -> str:
        """ç”ŸæˆSearXNGæ ¼å¼è§„åˆ™"""
        rules = []
        rules.append("# SearXNG hostname rules")
        rules.append("# Format: !domain.com")
        rules.append("")

        for domain in domains:
            if self.detector.analyze_domain(domain)['is_content_farm']:
                rules.append(f"!{domain}")

        return '\n'.join(rules)

    def generate_ublock_rules(self, domains: List[str]) -> str:
        """ç”ŸæˆuBlock Originæ ¼å¼è§„åˆ™"""
        rules = []
        rules.append("# uBlock Origin filter rules")
        rules.append("# Format: ||domain.com^")
        rules.append("")

        for domain in domains:
            if self.detector.analyze_domain(domain)['is_content_farm']:
                rules.append(f"||{domain}^")

        return '\n'.join(rules)

    def generate_hosts_file(self, domains: List[str]) -> str:
        """ç”Ÿæˆhostsæ–‡ä»¶æ ¼å¼"""
        rules = []
        rules.append("# Hosts file for blocking content farms")
        rules.append("# Format: 127.0.0.1 domain.com")
        rules.append("")

        for domain in domains:
            if self.detector.analyze_domain(domain)['is_content_farm']:
                rules.append(f"127.0.0.1 {domain}")

        return '\n'.join(rules)
```

## ğŸ¨ åŠŸèƒ½ç‰¹æ€§

### 1. æ™ºèƒ½è¯†åˆ«ç®—æ³•
- **URLæ¨¡å¼åŒ¹é…** - åŸºäºåŸŸåå’ŒURLç»“æ„è¯†åˆ«
- **å†…å®¹åˆ†æ** - åˆ†æé¡µé¢å†…å®¹ç‰¹å¾
- **æœºå™¨å­¦ä¹ ** - ä½¿ç”¨åˆ†ç±»ç®—æ³•æå‡è¯†åˆ«ç²¾åº¦
- **ç¤¾åŒºåé¦ˆ** - ç»“åˆç”¨æˆ·ä¸¾æŠ¥å’ŒéªŒè¯

### 2. å¤šç»´åº¦è¯„ä¼°
```python
class DomainEvaluator:
    def __init__(self):
        self.quality_factors = {
            'content_length': 0.1,
            'originality_score': 0.3,
            'ad_density': 0.2,
            'source_citations': 0.2,
            'user_engagement': 0.2,
        }

    def evaluate_quality(self, domain: str) -> Dict:
        """ç»¼åˆè¯„ä¼°åŸŸåå†…å®¹è´¨é‡"""
        scores = {}

        # å†…å®¹é•¿åº¦è¯„ä¼°
        scores['content_length'] = self.analyze_content_length(domain)

        # åŸåˆ›æ€§è¯„åˆ†
        scores['originality_score'] = self.check_originality(domain)

        # å¹¿å‘Šå¯†åº¦åˆ†æ
        scores['ad_density'] = self.analyze_ad_density(domain)

        # å¼•ç”¨æ¥æºæ£€æŸ¥
        scores['source_citations'] = self.check_citations(domain)

        # ç”¨æˆ·å‚ä¸åº¦
        scores['user_engagement'] = self.analyze_engagement(domain)

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        total_score = sum(
            score * self.quality_factors[factor]
            for factor, score in scores.items()
        )

        return {
            'domain': domain,
            'total_score': total_score,
            'detailed_scores': scores,
            'is_content_farm': total_score < 0.4
        }
```

### 3. è‡ªåŠ¨åŒ–æ›´æ–°æœºåˆ¶
```python
import schedule
import time

def update_filter_rules():
    """å®šæ—¶æ›´æ–°è¿‡æ»¤è§„åˆ™"""
    print("Starting filter rule update...")

    # è·å–æ–°çš„åŸŸååˆ—è¡¨
    new_domains = fetch_candidate_domains()

    # åˆ†æå’Œç”Ÿæˆè§„åˆ™
    detector = ContentFarmDetector()
    generator = FilterGenerator(detector)

    # ç”Ÿæˆå¤šç§æ ¼å¼
    searxng_rules = generator.generate_searxng_rules(new_domains)
    ublock_rules = generator.generate_ublock_rules(new_domains)
    hosts_rules = generator.generate_hosts_file(new_domains)

    # ä¿å­˜åˆ°æ–‡ä»¶
    with open('searxng_hostname.txt', 'w') as f:
        f.write(searxng_rules)

    with open('ublock_filter.txt', 'w') as f:
        f.write(ublock_rules)

    with open('hosts_block.txt', 'w') as f:
        f.write(hosts_rules)

    print("Filter rules updated successfully!")

# è®¾ç½®å®šæ—¶ä»»åŠ¡
schedule.every().day.at("02:00").do(update_filter_rules)

while True:
    schedule.run_pending()
    time.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
```

## ğŸ”§ é›†æˆæ–¹æ¡ˆ

### 1. SearXNGé›†æˆ
```python
# searxng settings.py é…ç½®
hostname_rules = {
    'replacement': None,
    'rules_file': '/path/to/searxng_hostname.txt',
    'name': 'Content Farm Blocker',
    'description': 'Block content farm websites'
}

# æ·»åŠ åˆ°æœç´¢å¼•æ“é…ç½®
engines = [
    {
        'name': 'google',
        'engine': 'google',
        'hostname_replace': hostname_rules,
        # ... å…¶ä»–é…ç½®
    }
]
```

### 2. uBlock Originé›†æˆ
```javascript
// è‡ªå®šä¹‰è¿‡æ»¤å™¨
const contentFarmFilter = {
    name: 'Content Farm Blocker',
    list: [
        '||example-content-farm.com^',
        '||low-quality-site.info^',
        '||clickbait-blog.top^'
    ],
    enabled: true
};

// æ·»åŠ åˆ°è¿‡æ»¤å™¨åˆ—è¡¨
function addCustomFilter() {
    const filters = [];
    filters.push(contentFarmFilter);
    return filters;
}
```

## ğŸ“Š æ•°æ®ç»Ÿè®¡

### è¿‡æ»¤æ•ˆæœåˆ†æ
```python
def generate_statistics():
    """ç”Ÿæˆè¿‡æ»¤ç»Ÿè®¡æŠ¥å‘Š"""
    stats = {
        'total_domains_analyzed': 0,
        'content_farms_blocked': 0,
        'false_positives': 0,
        'accuracy_rate': 0.0,
        'coverage_rate': 0.0
    }

    # è¯»å–è¿‡æ»¤æ—¥å¿—
    with open('filter_log.txt', 'r') as f:
        logs = f.readlines()

    for log in logs:
        if 'ANALYZED' in log:
            stats['total_domains_analyzed'] += 1
        elif 'BLOCKED' in log:
            stats['content_farms_blocked'] += 1
        elif 'FALSE_POSITIVE' in log:
            stats['false_positives'] += 1

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    if stats['total_domains_analyzed'] > 0:
        stats['coverage_rate'] = stats['content_farms_blocked'] / stats['total_domains_analyzed']
        stats['accuracy_rate'] = 1 - (stats['false_positives'] / stats['content_farms_blocked'])

    return stats
```

## ğŸš€ éƒ¨ç½²å’Œç»´æŠ¤

### è‡ªåŠ¨åŒ–éƒ¨ç½²
```yaml
# GitHub Actionså·¥ä½œæµ
name: Update Content Farm Filter

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯æ—¥æ›´æ–°
  workflow_dispatch:

jobs:
  update-filter:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 schedule

    - name: Update filter rules
      run: |
        python update_filters.py

    - name: Commit and push
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git commit -m "Auto update content farm filters - $(date)"
        git push
```

## ğŸ”® é¡¹ç›®ä»·å€¼

### æŠ€æœ¯ä»·å€¼
- **æœç´¢è´¨é‡æå‡** - æœ‰æ•ˆè¿‡æ»¤ä½è´¨é‡å†…å®¹
- **ç”¨æˆ·ä½“éªŒæ”¹å–„** - å‡å°‘ä¿¡æ¯å™ªéŸ³å’Œè¯¯å¯¼å†…å®¹
- **å¼€æºè´¡çŒ®** - ä¸ºæœç´¢å¼•æ“ç”Ÿæ€æä¾›ä»·å€¼
- **å·¥å…·é›†æˆ** - æ”¯æŒå¤šç§å¹³å°å’Œå·¥å…·

### ç¤¾ä¼šä»·å€¼
- **ä¿¡æ¯å‡€åŒ–** - æå‡ç½‘ç»œä¿¡æ¯ç¯å¢ƒè´¨é‡
- **æ•™è‚²ä»·å€¼** - å¸®åŠ©ç”¨æˆ·è¯†åˆ«å’Œé¿å…å†…å®¹å†œåœº
- **æ•ˆç‡æå‡** - èŠ‚çœç”¨æˆ·ç­›é€‰ä¿¡æ¯çš„æ—¶é—´
- **çŸ¥è¯†ä¿æŠ¤** - ä¿æŠ¤åŸåˆ›å’Œé«˜è´¨é‡å†…å®¹çš„å¯è§æ€§

---

**é¡¹ç›®é“¾æ¥**: [GitHub Repository](https://github.com/hezhijie0327/ContentFarm4SearXNG)

**æŠ€æœ¯æ ˆ**: Python | SearXNG | Content Analysis | Machine Learning | Web Scraping